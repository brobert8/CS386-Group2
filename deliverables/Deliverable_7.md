## Deliverable 7: Verification & Validation
By: Ian ambos, Peter Galvan, Chase Spigarelli, Nick Nannen, Brandon Roberts and Jonathan White Velasco

## 1. Description
Provide 1-2 paragraphs to describe your system. This will help us to remember what your system is about. 

- Stress and heavy workload impacts students who need their papers peer reviewed. Often, the consequences include unverified papers and 
documents that have many errors. Peer Pad helps match writers with peers at a similar academic skill level. This way they can receive relevant 
feedback on their papers and projects. Peer Pad is also a great space for people who are interested in reading interesting papers for free. The system 
is different from a lot of other crowdsourcing websites. This system is efficient for students who may be too shy to share, in a hurry or needing 
different perspectives in a peer review. We are focused on letting students post their paper and have them reviewed by people as quickly as possible.

Grading: 1.0 point. Criteria: Completeness (0.5); Language (0.5)

## 2. Verification (tests) 
Verification aims to ensure that you correctly developed the product.

### 2.1. Unit test (Missing whole section don’t understand testing)
A unit test is an automated test that aims to verify the behavior of a component isolated from the rest of the system. For this deliverable, you should have 
automated tests for the main components of your project. Provide the following information:

2.1.1. Test framework you used to develop your tests (e.g., JUnit, unittest, pytest, etc.): 
- Temp bullet point

2.1.2. Link to your GitHub folder where your automated unit tests are located.
- Temp bullet point

2.1.3. An example of a test case that makes use of mock objects. Include in your answer a GitHub link to the class being tested and to the test.
- Temp bullet point

2.1.4. A print screen showing the result of the unit tests execution.
- Temp bullet point

Grading: 5.0 points. Criteria: Adequate choice of a test framework (0.5); Coverage of the tests (1.5); Quality of the tests (1.0); Adequate use of 
Mock objects (1.0); Print screen showing successful tests execution (1.0).

### 2.2. Integration test 
An integration test is an automated test that verifies the implementation of a business rule that involves multiple components with the goal of
certifying that they work together to produce the expected result. They are often performed in the same test platform as the unit tests. 
Provide the following information:


2.2.1. Test framework you used to develop your tests:
- Temp bullet point

2.2.2. Link to your GitHub folder where your automated integration tests are located.
- Temp bullet point

2.2.3. An example of an integration test. Include in your answer a GitHub link to the test and an explanation about what parts of the system are being exercised by this test. 
- Temp bullet point

2.2.4. A print screen showing the result of the integration tests execution
- Temp bullet point

Grading: 4.0 points. Criteria: Adequate choice of a test framework (0.5); Coverage of the tests (0.5); Quality of the tests (1.0); Adequate example of an 
integration test (1.5); Print screen showing successful tests execution (0.5).

### 2.3. Acceptance 
An acceptance test is a test that verifies the correct implementation of a feature from the user interface perspective. An acceptance test is a black box test 
(the system is tested without knowledge about its internal implementation). Provide the following information: 

2.3.1. Test framework you used to develop your tests (e.g., Selenium, Katalon Studio, Espresso2, Cucumber, etc.):
- Temp bullet point

2.3.2. Link to your GitHub folder where your automated acceptance tests are located.
- Temp bullet point

2.3.3. An example of an acceptance test. Include in your answer a GitHub link to the test and an explanation about the tested feature.
- Temp bullet point

2.3.4. A print screen/video showing the acceptance test execution.
- Temp bullet point

Grading: 5.0 points. Criteria: Adequate choice of a test framework (1.0); Coverage of the tests (0.5); Quality of the tests (1.0); Adequate example of an 
acceptance test (1.5); Print screen/video showing successful tests execution (1.0). 

## 3. Validation (user evaluation) 
Validation aims to ensure that you developed the right product. You started the software inception by talking to users and stakeholders. Now it is time 
to check if you are on the right track by conducting some user evaluation on the actual system. Include in this deliverable the following information:

Script: The script should have the tasks that you will give to the user, what you are going to collect, and what you are going to ask. Do not forget to add 
questions about the users’ general impressions. You can ask open questions (e.g., How would you describe the homepage of our app? How do you compare our 
system to the competitor X?) or closed questions (On a scale of 1 to 10, how would you rate the layout of our application? On the same scale, how likely 
would you use the system in its current state?). Take a look at the inception and requirements deliverables to help create the script (aim to check if 
you are achieving your initial goals and if the features are implemented in a satisfactory way).

- Task 1: Turn in a paper (.txt only)
- Task 2: Get a paper to grade
- Task 3: Return the paper you just graded
- Task 4: get that paper back you just graded
- Task 5 (optional): Come back in a couple of days to try to receive your paper to see if it was graded. 

User one name: Alysa Rippee
- Questions
  - On a scale of 1 to 10, how would you rate the layout of our application? Why? 
    - I would rate it a 7. It was kind of confusing to navigate, but once explained, very simple through the layout. 
  - On the same scale, how likely would you use the system in its current state? Why?
    - I would rate it a 6 or a 7. The idea is super cool, but I don’t think it is super use-friendly to people who aren’t super great at navigating technology.
  - What was difficult to navigate? 
    - I would recommend, for the time being, let users know that they can only upload .txt. I could see that deterring people when they try to submit something
      and get an error that would be difficult to navigate if it was not explained in the tasks. The buttons/order to do things was a little confusing to follow. 
      For example, when I chose my file to be graded, I thought it would automatically upload, but there was another button/step I missed.
  - What would you improve? 
    - Look into accepting more types of files like .docx and others. Maybe a more user-friendly layout.
  - What is the overall score of the product? Why?  
    - I would give the overall score of the product a 7. I think the idea really carries the product, but it could use some layout improvements. Although, I 
      think the idea is super innovative and exciting.
      
User two name: Jimmy Carpenter
- Questions
  - On a scale of 1 to 10, how would you rate the layout of our application? Why? 
    - 7 because I like the way it looks, but it was difficult to navigate the steps
  - On the same scale, how likely would you use the system in its current state? Why?
    - 3 because it was difficult to navigate.
  - What was difficult to navigate? 
    - The steps need to be laid out more clearly.
  - What would you improve? 
    - I think the page should have each step listed so you know which button to click on.
  - What is the overall score of the product? Why?  
    - 4 because I think it needs to be much more user friendly for people that do not understand txt files.

User three name: Jimmy Carpenter
- Questions
  - On a scale of 1 to 10, how would you rate the layout of our application? Why? 
    - 3/10. On my 13 inch MacBook Pro the UI didn’t display correctly and all of the features were overlapping.
  - On the same scale, how likely would you use the system in its current state? Why?
    - 2/10. As mentioned above, the glitching UI made it difficult to navigate the site correctly. 
  - What was difficult to navigate? 
    - At times I found it difficult to interpret each of the buttons and their functionalities which made completing the grading process difficult.
  - What would you improve? 
    - Overall, I would focus on improving the UI, and making the site more user friendly and intuitive. 
  - What is the overall score of the product? Why?  
    - 2.5/10. Very difficult to use and navigate in its current state. 

Overall reflection on user evaluation:
- There are a couple things that we noticed that we could change based on the user's interaction with the product and feedback. A big bug we caught on the 
last user was he was using a Macbook with only a 13-inch screen and everyone that programmed for the product uses a 27-inch display. The UI from his end was 
over lapping and wasn’t really usable. Also, for submitting a paper and returning a paper we should only have one button. If possible, we shouldn’t make them 
get the paper then have a separate button for uploading. All users thought their paper was uploaded after just getting the paper. Also, people don’t do a lot 
of writing in .txt. We should really add support for .docx because almost every user had no clue what .txt was. We had to help them even create one. Next, they 
were even confused on how to obtain a file for grading. We think we could make different pages for different groups of tasks. Instead of having all the buttons 
on one page. For example: a page for grading, getting papers back and submitting papers for review. However, to draw something positive most of them understood 
the layout and how it worked after it was explained. They thought it was very simple. They even like the design of the site as a whole. They also said that the 
idea and the functions are there, but a more user friendly UI would bolster the project. It was something they could see themselves using in the future, just not 
today with how unfriendly the UI is. However, the concept is interesting and they see potential in the product. We feel that the product meets the value proposition. 
However, there is some serious work that needs to go into fixing the UI.

Grading: 10.0 points. Criteria: Adequate script (3.0); Adequate report of the results (3.0); Adequate reflection (3.0); Language (1.0)
